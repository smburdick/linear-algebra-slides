\documentclass{beamer}
%Information to be included in the title page:
\title{Advanced Linear Algebra for\\Quantum Error Correction}
\author{Sam Burdick}
\institute{Topological Quantum Error Correction
}
\date{\today}

\usepackage{ragged2e}
\usepackage{multicol}

\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tr}{\text{tr}}
\newcommand{\B}{\mathcal B}
\newcommand{\F}{\mathbb F}
\renewcommand{\O}{\mathcal O}
\newcommand{\Var}{\text{Var}}
\newcommand{\CNOT}{\text{CNOT}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\vec}{\textbf}

\usepackage{braket}
\usepackage{hyperref}

%\usepackage{tikz}
%\usetikzlibrary{quantikz2}

\hypersetup{
    colorlinks=true,
    linkcolor=red,
    citecolor=green,
    filecolor=magenta,
    urlcolor=blue
}

%\usefonttheme{serif}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Agenda}
\begin{multicols}{2}
    \begin{enumerate}
    \item Vector spaces
    \item Basis vectors
    \item Linear maps
    \item Change of basis
    \item Dirac notation
    \item Subspaces
    \item Orthonormal basis
    \item Eigenstates
    \item Unitary matrices
    \item The Kronecker product
    \item The commutator
    \item Matrix trace
    \item Projection operators
    \item Clifford gate conjugation
    \item Stabilizers
    \item A five-qubit code
    \item Spectral Theorem
    \item * Linear functionals and dual spaces
    \item * Riesz Representation Theorem
    \item * Application: the uncertainty principle
    \end{enumerate}
\end{multicols}
* indicates supplemental content
\end{frame}

\begin{frame}{To the reader}
We assume existing familiarity with sets, functions, matrices, vectors, summation notation, probabilities, and complex numbers. I recommend Dr. Beezer's free textbook \href{http://linear.ups.edu}{A First Course in Linear Algebra} to newcomers. We also assume familiarity with the mathematical basics of quantum error correction (including quantum gates and the Pauli matrices) which is given in Dr. Fowler's \href{https://www.coursera.org/learn/quantum-error-correction}{free Coursera course on the subject}.
\begin{flushright}
    \textit{Narrative is linear, but action has breadth\\and depth as well as height and is solid.}\\---Thomas Carlyle
\end{flushright}

\begin{flushright}
    \textit{I discovered that the library is the real school.}\\---Ray Bradbury
\end{flushright}
\end{frame}

\begin{frame}{Vector spaces}

\begin{block}{Definition}
$V$ is a vector space if it is a set of vectors coupled with the addition of vectors and scalar multiplication.

Vector addition: $\textbf u,\textbf v\in V$ means that $\textbf u+ \textbf v \in V$.

Scalar multiplication: $\alpha \textbf v \in V$ for any $\textbf v \in V,\alpha \in \C$.
\end{block}

\begin{block}{Example}
An $n$-qubit system's state vector occupies the vector space $\C^{2^n}$. A single qubit comprises the state vector $\binom{\alpha}{\beta}$, where $\alpha$ and $\beta$ are complex numbers satisfying $|\alpha|^2+|\beta|^2=1$.
\end{block}

\begin{block}{Remark}
    Vector spaces have numerous other properties that show that vector space operations (vector addition and scalar multiplication) obey commutativity and associativity laws; they can be found \href{http://linear.ups.edu/html/section-VS.html}{here}.
\end{block}

\end{frame}

\begin{frame}{Linear independence and basis vectors}

\begin{block}{Definition}
    Given a set of vectors $S=\{\textbf{v}_1,\ldots, \textbf{v}_m\}$ we say that $S$ is linearly independent if $$ \sum_{k=1}^m \alpha_k \textbf{v}_k = \textbf{0}, $$ where $\textbf{0}$ is the zero vector, if $\alpha_k = 0$ for all $1 \leq k \leq m.$
\end{block}

\begin{block}{Definition}

If a set of vectors $T$ can be combined in a linear fashion to produce every element of $S$, we say that $T$ spans $S$.

\end{block}

\begin{block}{Definition}
Suppose $V$ is a vector space. Then a minimum cardinality subset $\mathcal{B} \subseteq V$ that is linearly independent and spans $V$ is a basis of $V$.
\end{block}
    
\end{frame}

% \begin{frame}{Linear independence and basis vectors (cont'd)}
%     \begin{block}{Example}
%         In quantum computing, the most common bases for a single 
%     \end{block}
% \end{frame}

\begin{frame}{Linear maps}

\begin{block}{Definition}
A linear map is a function $T:U\to V$, where $U$ and $V$ are vector spaces, such that
$$ T(\textbf{u}_1 + \textbf{u}_2) = T(\textbf{u}_1) + T(\textbf{u}_2)\text{ for all } \textbf{u}_1,\textbf{u}_2\in U$$
and
$$ T(\alpha \textbf u) = \alpha T(\textbf u) \text{ for all } \textbf u\in U,\alpha \in \C.$$
\end{block}

\begin{block}{Example}
The Hadamard gate
$ H = \frac{1}{\sqrt 2} \begin{pmatrix}
    1 & 1 \\
    1 & -1
\end{pmatrix}$
is a linear map $H:\C^2 \to \C^2$ acting on a single qubit $\binom{\alpha}{\beta}$.
\end{block}
    
\end{frame}

\begin{frame}{Linear maps (cont'd)}
    \begin{block}{Remark}
        In quantum computing, operators and observables are both represented as linear maps and $2^n \times 2^n$ matrices that act on state vectors of dimension $2^n$.
    \end{block}
    \begin{block}{Exercise}
        Show that the Pauli operators $X$ and $Z$ as well as the Clifford-$T$ gates $\CNOT$, $H$, $S$ and $T$ are linear maps.
    \end{block}
    \begin{block}{Exercise}
        Linearity is also a property of sets of certain functions over other fields, including $\R$ and $\C$. Suppose $f(x)$ is differentiable for all $x \in \R$. Show that differentiation and integration of $f(x)$ are linear maps over sets of all such $f(x)$.
    \end{block}
    \begin{flushright}
        \textit{The map is not the territory.}\\---Alfred Korzybski
    \end{flushright}
\end{frame}

\begin{frame}{Change of basis}
    \begin{block}{Definition}
        Given two vector spaces $V$ and $W$, each of dimension $n$, the change of basis matrix is an $n\times n$ matrix $\mathcal C$. Let $\mathcal V$ and $\mathcal W$ denote matrices comprised of the basis vectors of $V$ and basis vectors $W$ expressed in terms of the basis vectors of $V$, respectively. Then $\mathcal V = \mathcal C\mathcal W\mathcal C^{-1}$.
    \end{block}
    \begin{block}{Exercise}
        Verify that the Hadamard matrix $H$ functions as a change of basis matrix between vectors in the $X$ basis and the $Z$ basis, and vice versa.
    \end{block}
    \begin{flushright}
        \textit{Change in all things is sweet.}\\---Aristotle
    \end{flushright}
\end{frame}

\begin{frame}{Dirac notation}

\begin{block}{Definition}
A ket (or state vector) $\ket{\psi}$ is a normalized vector in the vector space $\C^n$, meaning that $\sum_{k=1}^n |\psi_k|^2 = 1$  and $\psi_k$ is the $k$th element of $\ket\psi.$
\end{block}

\begin{block}{Definition}
A bra-ket $\braket{\phi|\psi}$ is the inner product of $\ket\phi$ and $\ket\psi$, $$ \braket{\phi|\psi} = \sum_{k=1}^n \phi_k^* \psi_k , $$ where the bra $\bra\phi$ acts as a functional map (from a ket to a scalar; more on this later) on $\ket\psi$. The bra is the conjugate transpose of the corresponding ket.
\end{block}
\end{frame}

\begin{frame}{Dirac notation (cont'd)}

\begin{block}{Theorem (Cauchy-Schwarz Inequality)}
For any two state vectors $\ket\phi$ and $\ket\psi$, we have $ | \braket{\phi | \psi}|^2 \leq 1. $
\end{block}
    
\begin{block}{Definition}
A ket-bra $ \ket{\phi}\bra\psi $ is a linear map (or outer product)
$$
 \ket{\phi} \bra{\psi} = 
    \begin{pmatrix}
        \phi_1 \\
        \vdots \\
        \phi_n
    \end{pmatrix}
    \begin{pmatrix}
        \psi_1^*, \ldots,\psi_n^*
    \end{pmatrix} =
    \begin{pmatrix}
            \phi_1\psi_1^* & \ldots & \phi_1\psi_n^* \\
            \vdots & \ddots & \vdots \\
            \phi_n\psi_1^* & \ldots & \phi_n\psi_n^*
    \end{pmatrix}
$$
\end{block}

\begin{block}{Remark}
    Given a linear operator $A$ and bra $\bra\phi$, $\bra\phi A$ is also a bra defined by the function composition rule $$ (\bra\phi A) \ket\psi = \bra\phi (A\ket\psi) = \bra\phi A \ket\psi $$
\end{block}
\end{frame}

\begin{frame}{Dirac notation (cont'd)}
    \begin{block}{Exercise}
        Demonstrate the Cauchy-Schwarz inequality for the state vectors $\ket +$ and $\ket -$.
    \end{block}
    \begin{block}{Exercise}
        Compute the operator $\ket 0 \bra 1$.
    \end{block}
    \begin{block}{Exercise}
        Compute the ket $\bra - H \ket +$.
    \end{block}
    \begin{block}{Exercise}
        Show that for any state vector $\ket\psi$ that $\braket{\psi | \psi} = 1.$ (Use the identity $zz^* = |z|^2$ for all $z\in \C.$)
    \end{block}
    \begin{flushright}
        \textit{Pick a flower on Earth and you move the farthest star.}\\---Paul Dirac
    \end{flushright}
\end{frame}

\begin{frame}{Subspaces}
    \begin{block}{Definition}
        A set $W$ is a subspace of a vector space $V$ if $W$ is itself a vector space and $W\subseteq V$.
    \end{block}
    % \begin{block}{Example}
    %     The vector space $$W = \left\{ \begin{pmatrix} a_0 \\ a_1 \\ a_2 \\ 0 \end{pmatrix}: a_0,a_1,a_2 \in \C \right \}$$ is a subspace of the vector space $\C^4$.
    % \end{block}
    \begin{block}{Example}
        Consider the Bell pair $ \ket{\Phi^+}= \frac{\ket{00} + \ket{11}}{\sqrt2} $. The state vector $\ket{\Phi^+}$ is a member of the (maximally entangled) subspace defined as
        $$ B= \left\{ \alpha\ket{00} + \beta\ket{11} : \alpha,\beta \in \C,\text{ } |\alpha|^2 + |\beta|^2 = 1 \right\}. $$
    \end{block}
    \begin{block}{Exercise}
        Show that $B$ is a vector space, and then show that $B\subseteq \C^4$.
    \end{block}
    \begin{flushright}
        \textit{Happiness is a vector, it is movement.}\\---Neal Shusterman
    \end{flushright}
\end{frame}


\begin{frame}{Orthonormal basis}

\begin{block}{Definition}
    An orthonormal basis is a set of basis vectors $\B$ in a vector space with a Euclidean norm of $1$, meaning $\sqrt{\sum_{k=1}^n |\psi_k|^2} = 1$ for all $\ket\psi \in \B$, and every pair of distinct vectors $\ket\phi,\ket\psi \in \B$ is orthogonal, meaning $\braket{\phi | \psi} = 0$.
\end{block}

% \begin{block}{Example}
%     The Euclidean basis vectors form an orthonormal basis of $\C^n$; for $n=4$ they are $$ \left \{ \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix} \right \} $$ 
% \end{block}

\begin{block}{Exercise}
    Show that the sets $\{ \ket0 ,\ket1 \}$ and $ \{ \ket+, \ket-\}$ are both orthonormal bases of the single-qubit vector space $\C^2$.
\end{block}

\begin{flushright}
    \textit{The orthogonal features, when combined,\\can explode into complexity.}\\---Yukihiro Matsumoto
\end{flushright}

\begin{flushright}
    \textit{The endeavor to understand is the first and only basis of virtue.}\\---Baruch Spinoza
\end{flushright}

\end{frame}
% \begin{frame}{Orthonormal basis (cont'd)}

% \begin{block}{Exercise (ChatGPT)}
% Consider the 3-qubit bit-flip code with stabilizer generators $Z_1Z_2$ and $Z_2Z_3$.
% \begin{itemize}
%     \item Identify the code space and give an orthonormal basis for it.
%     \item  Apply the single-qubit error operators $X_1$, $X_2$, and $X_3$ to the basis states of the code space to generate the corresponding error spaces.
%     \item Show that each error space is a two-dimensional subspace orthogonal to the code space and to each other.
%     \item Verify that the set of all these subspaces forms an orthogonal decomposition of the full 3-qubit vector space.
% \end{itemize}
% \end{block}

% \end{frame}

\begin{frame}{Eigenstates}
\begin{block}{Definition}
A state vector $\ket\psi$ is the $\lambda$-eigenstate (or eigenvector) of a linear map $U$ if $U\ket\psi = \lambda \ket\psi$ for some $\lambda \in \R,$ where $\lambda$ is said to be an eigenvalue of $U$ (and corresponds to a measurement of a qubit).
\end{block}

\begin{block}{Example}
If $U\ket\psi = \ket\psi$, we say that $\ket\psi$ is the $+1$ eigenstate of $U$.
\end{block}


\begin{block}{Exercise}
Show that $\ket+$ and $\ket-$ are the $+1$ and $-1$ eigenstates of $X = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$, respectively.
% \newline
% \textit{Solution.}
% $$ X\ket+ = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \frac{1}{\sqrt2} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{1}{\sqrt2}\begin{pmatrix} 1 \\ 1 \end{pmatrix} = \ket+.$$
\end{block}

\begin{block}{Exercise}
Show that $\ket0$ and $\ket 1$ are the $+1$ and $-1$ eigenstates of $Z = \begin{pmatrix}
    1 & 0 \\ 0 & -1
\end{pmatrix}$, respectively.
\end{block}

\end{frame}
\begin{frame}{Unitary matrices}

\begin{block}{Definition}
    The conjugate of a matrix $U$ is denoted $U^*$ and obtained by conjugating each of its elements.
\end{block}


\begin{block}{Definition}
The transpose of a matrix $U$ is denoted $U^T$ and is obtained by systematically exchanging (i.e., swapping) the values of the rows with the values in the columns in $U$.
\end{block}

\begin{block}{Definition}
    The adjoint of a matrix $U$, denoted $U^\dagger$, is the conjugate transpose $$ U^\dagger = (U^*)^T = \begin{pmatrix}
        u_{1,1}^* & \cdots & u_{1,n}^* \\
        \vdots & \ddots & \vdots \\
        u_{n,1}^* & \cdots & u_{n,n}^*
    \end{pmatrix}^T = \begin{pmatrix}
        u_{1,1}^* & \cdots & u_{n,1}^* \\
        \vdots & \ddots & \vdots \\
        u_{1,n}^* & \cdots & u_{n,n}^*
    \end{pmatrix}$$ 
\end{block}
\end{frame}

\begin{frame}{Unitary matrices (cont'd)}

\begin{block}{Definition}
A matrix $U$ is Hermitian if it is self-adjoint, that is, $U=U^\dagger.$
\end{block}

\begin{block}{Definition}

A matrix $U$ is unitary if $UU^\dagger = U^\dagger U = I.$

\end{block}

\begin{block}{Exercise}
    Show that the $S$ and $T$ gates are unitary but not Hermitian.
\end{block}

\begin{block}{Exercise}
Show that every quantum operator is unitary.

\textit{Hint.} Use the fact that $\braket{\psi | \psi} = 1$ and substitute $\ket\psi$ for $U\ket\psi$ for an arbitrary operator $U$. Then use function composition rules.

\end{block}

\end{frame}

\begin{frame}{The Kronecker product}

\begin{block}{Definition}
    
For two-dimensional, normalized quantum states $\ket\phi$ and $\ket\psi$, the Kronecker product (or tensor product) between them is

$$ \ket\phi  \otimes \ket\psi = \begin{pmatrix} \phi_1 \\ \phi_2 \end{pmatrix} \otimes \begin{pmatrix} \psi_1 \\ \psi_2 \end{pmatrix} = \begin{pmatrix}
    \phi_1 \begin{pmatrix}\psi_1 \\ \psi_2 \end{pmatrix} \\
    \phi_2 \begin{pmatrix}\psi_1 \\ \psi_2 \end{pmatrix}
\end{pmatrix} =
\begin{pmatrix}
    \phi_1\psi_1 \\
    \phi_1\psi_2 \\
    \phi_2\psi_1 \\
    \phi_2\psi_2
\end{pmatrix}
$$
and is sometimes denoted $\ket\phi\ket\psi$ or $\ket{\phi\psi}$.
\end{block}

\begin{block}{Remark}
You can take Kronecker products of matrices as well; the new object's dimension is the product of the dimensions of the objects multiplied, meaning that Kronecker products of higher-dimensional objects quickly become unmanageable to compute by hand.
\end{block}

\end{frame}

\begin{frame}{The Kronecker product (cont'd)}
    \begin{block}{Definition}
        The Kronecker product of two $2\times 2$ matrices $A$ and $B$ can be computed as
        $$ A \otimes B = \begin{pmatrix}
            a_{11} B & a_{12}B \\ a_{21}B & a_{22}B
        \end{pmatrix}, $$ which is a $4\times4$ matrix.
    \end{block}
    \begin{block}{Exercise}
        Compute the Kronecker products $X\otimes X$ and $T\otimes T$, where $T=\begin{pmatrix}
            1 & 0 \\ 0 & e^{i\pi/4}
        \end{pmatrix}$.
    \end{block}
    \begin{block}{Remark}
        Stabilizers are Kronecker products of many Pauli operators, for example $XXXX$ or $ZZZZ$ which have a more compact binary representation we'll explore later.
    \end{block}
    \begin{flushright}
        \textit{God created the integers; all else is the work of man.}\\---Leopold Kronecker
    \end{flushright}
\end{frame}

\begin{frame}{The commutator}

\begin{block}{Definition}
    Two matrices $A$ and $B$ are said to commute if $AB=BA$.
\end{block}

\begin{block}{Remark}
For quantum operators $A$ and $B$, $(AB)\ket\psi$ means ``apply $B$ first, then $A$, on $\ket\psi$.''
\end{block}

\begin{block}{Definition}
    The commutator of two matrices is defined as $[A,B]= AB-BA$.
\end{block}

\begin{block}{Corollary}
    For commuting matrices, $[A,B]=\mathcal O$ (the zero matrix).
\end{block}

\begin{block}{Exercise}
Show that $[X,Z] = -2iY$, where $Y=\begin{pmatrix}
    0 & -i \\ i & 0
\end{pmatrix}$.
\end{block}

\begin{flushright}
    \textit{... policies that can make commuting shorter ... would be a straightforward way to reduce minor but widespread suffering.}\\---Daniel Kahneman
\end{flushright}
    
\end{frame}


\begin{frame}{Matrix trace}

\begin{block}{Definition}
The trace of an $n \times n$ matrix $A$ is the sum of its diagonal elements; that is $$\tr(A) = \sum_{k=1}^n A_{k,k}$$
\end{block}

\begin{block}{Remark}
Trace is ``commutativity-preserving,'' meaning $\tr(AB)=\tr(BA)$ for any matrices $A,B$.
\end{block}

\begin{block}{Remark}
The trace is independent of a chosen basis, meaning that if a linear map $A$, represented by a matrix, has a change of basis matrix $P$ such that $B=P^{-1}AP$, then $\tr(A)=\tr(B)$.
\end{block}

\end{frame}

\begin{frame}{Matrix trace (cont'd)}
    \begin{block}{Exercise}
        Show that $\tr(P)=0$ for each non-identity Pauli operator $P$.
    \end{block}
    \begin{block}{Exercise}
        Show that $\tr(XZ)=\tr(ZX)$.
    \end{block}
    \begin{flushright}
    \textit{It is by tracing things to their origin,\\that we learn to understand them ...}\\---Thomas Paine
\end{flushright}
\end{frame}

\begin{frame}{Projection operators}
    \begin{block}{Definition}
A Hilbert space $\mathcal H$ in quantum computing is a complex inner product space, that is, a vector space over $\C^n$ endowed with an inner product operation.
\end{block}
    \begin{block}{Definition}
        A projection operator, denoted $\hat P$, when applied to a quantum state $\ket\psi \in \H$, where $\H$ is a Hilbert space, returns a vector in a subspace of $\H$ defined with respect to a specific state $\ket a$, and is defined as $$ \hat P_a = \ket a\bra{a}, $$ where $\ket a$ is in an orthonormal basis of $\H$.
    \end{block}
    \begin{block}{Remark}
        Projection operators are used to model quantum measurement; in quantum computing, they project a qubit into a specific basis, such as the $X$ or $Z$ basis (where only one of two outcomes are possible).
    \end{block}
\end{frame}

\begin{frame}{Projection operators (cont'd)}
    \begin{block}{Remark}
    Projection operators are idempotent, meaning that $\hat P^2 = \hat P$, and Hermitian, meaning that $\hat P^\dagger = \hat P$.
    \end{block}
    \begin{block}{Example}
        For a single qubit in the general state $\ket\psi=\alpha\ket0 + \beta\ket1$, a measurement in the $Z$ basis $\{\ket0,\ket1\}$ is described by two projection operators:
        \begin{itemize}
            \item Projection operator for outcome $\ket 0: \hat P_0 = \ket 0\bra0$; note that $p(0) = |\alpha|^2$
            \item Projection operator for outcome $\ket 1: \hat P_1 = \ket1\bra1$; note that $p(1) = |\beta|^2$
        \end{itemize}
    \end{block}
    \begin{block}{Remark}
        The probability of measuring $\ket\psi$ as $\ket a$ can be computed as $$ p(a) = |\braket{a|\psi}|^2  = \bra \psi \hat P_a\ket\psi.$$
    \end{block}
\end{frame}

\begin{frame}{Projection operators (cont'd)}
    \begin{block}{Exercise}
        Compute the matrices $\hat P_0$ and $\hat P_1$, then show that any projection operator is not unitary.
    \end{block}
    \begin{block}{Definition}
        The CNOT gate has the following matrix representation: $$ \CNOT = \begin{pmatrix}
            I & \O \\ \O & X
        \end{pmatrix} = \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1 \\
            0 & 0 & 1 & 0
        \end{pmatrix}. $$
    \end{block}
    \begin{block}{Exercise}
        Show that the conditional action of the CNOT gate can be written as $$ \CNOT = (\ket0\bra0\otimes I) + (\ket1\bra1\otimes X).$$
    \end{block}

\end{frame}

\begin{frame}{Clifford gate conjugation}
    \begin{block}{Definition}
        A Clifford operator $C$ ``conjugates within'' the Pauli operators $ \{ I, X, Y, Z\}$; that is: $$ P' = CPC^\dagger $$ where $P$ and $P'$ are Kronecker products of Pauli operators in equal dimension to $C$.
    \end{block}
    \begin{block}{Remark}
        Any Clifford operator can be generated from Hadamard, $\CNOT$, and/or $S$ operators. The Pauli operators are themselves Clifford operators. The ``Clifford + $T$'' gates suffice to construct any quantum circuit.
    \end{block}
    \begin{flushright}
        \textit{In our implementation of a quantum [CNOT] logic gate, the target qubit $\ket S$ is spanned by two $^2 S_{1/2}$ hyperfine ground states of a single $^9\textnormal{Be}^+$ ion ...}\\---C. Monroe \textit{et al} [1995]
     \end{flushright}
\end{frame}

\begin{frame}{Clifford gate conjugation (cont'd)}
        \begin{block}{Example}
        The movement of an $X$ error through the control of a CNOT gate looks like:
        %$$
        %\begin{quantikz}
          %  &\push{X}&\ctrl{1}&\\
            %&&\targ{}&
        %\end{quantikz}=\begin{quantikz}
           % &\ctrl{1}&\push{X}&\\
            %&\targ{}&\push{X}&
        %\end{quantikz}
        %$$
        \begin{center}
        \includegraphics[scale=0.2]{X_CNOT.jpeg}
        \end{center}
        Using stabilizer notation, we represent this as $$(X_1 I_2) \xrightarrow{\CNOT_{1,2}} (X_1 X_2).$$ To demonstrate, let's use gate conjugation. The Clifford gate is $\CNOT$ and the Pauli operators are $X$ and $I$:
        $$ \CNOT(X\otimes I)\CNOT^\dagger = \begin{pmatrix}
            I & \O \\ \O & X
        \end{pmatrix}\begin{pmatrix}
            \O & I \\ I & \O
        \end{pmatrix}\begin{pmatrix}
            I & \O \\ \O & X
        \end{pmatrix} = X \otimes X $$
    \end{block}
\end{frame}

\begin{frame}{Clifford gate conjugation (cont'd)}
    \begin{block}{Exercise}
        Use Clifford gate conjugation to demonstrate the following error propagation rules:
        $$ X \xrightarrow{H} Z \xrightarrow{H} X $$
        $$ X \xrightarrow{S} Y \xrightarrow{S} -X $$
        $$ I_1Z_1 \xrightarrow{\CNOT_{1,2}} Z_1Z_2 $$
        $$ I_1X_2 \xrightarrow{\CNOT_{1,2}} I_1X_2$$
    \end{block}
    \begin{flushright}
        \textit{Still round the corner there may wait\\
        A new road or a secret gate.}\\---J. R. R. Tolkien, \textit{The Hobbit}
    \end{flushright}
\end{frame}

\begin{frame}{Stabilizers}
    \begin{block}{Definition}
        Each of the four Pauli operators can be encoded symplectically, using two bits as such:
        $$ I \to (0|0),\text{ } X\to (1|0), \text{ } Y\to(1|1), \text{ }Z\to(0|1). $$
    \end{block}
    \begin{block}{Definition}
        A stabilizer comprises the Kronecker product of $n$ Pauli operators represented in symplectic notation, written as a vector in block format as
        $$ \vec v = (x_1\cdots x_n | z_1 \cdots z_n) \in \F_2^{2n}.$$
        The left side of the vector is the $X$ sector and the right side is the $Z$ sector. $\F_2$ is the binary field $\{0,1\}$ (also denoted $GF(2)$), making $\F_2^{2n}$ a vector space over $\F_2$ of dimension $2n$.
    \end{block}

\end{frame}

\begin{frame}{Stabilizers (cont'd)}
    \begin{block}{Example}
        The 4-qubit stabilizer with Pauli string $XXXX$ can be represented as the bit vector $(1111|0000)$, and $XZXZ$ is $(1010|0101)$.
    \end{block}
    \begin{block}{Definition}
        The symplectic inner product of two vectors $\vec u, \vec v \in \F_2^{2n}$ is defined as
        $$ \vec u \odot \vec v = \sum_{k=1}^n \left [ x_k^{(\vec u)} z_k^{(\vec v)} + z_{k}^{(\vec v)}x_k^{(\vec u)}  \right] \mod 2, $$ where $x_k^{(\vec u)}$ denotes the $k$th element of the $X$ sector of $\vec u$, and so on.
    \end{block}
    \begin{block}{Theorem} % Gottesman 97?
        Two stabilizers represented symplectically by $\vec u$ and $\vec v$ commute if and only if $\vec u \odot \vec v = 0$.
    \end{block}
\end{frame}

\begin{frame}{A five-qubit code}
    \begin{block}{Example}
        We can use $m=4$ stabilizers to perform error detection using $n=5$ physical qubits. Let's tabulate:
        \begin{center}
            \begin{tabular}{c|c|c|c}
                Stabilizer & Pauli string & $X$ bits ($H_X)$ & $Z$ bits ($H_Z$) \\
                \hline
                 $S_1$ & $XZZXI$ & 10010 & 01100 \\
                 $S_2$ & $IXZZX$ & 01001 & 00110 \\
                 $S_3$ & $XIXZZ$ & 10100 & 00011 \\
                 $S_4$ & $ZXIXZ$ & 01010 & 10001
            \end{tabular}
        \end{center}
    This encodes $k=n-m=1$ logical qubit.
    \end{block}
    \begin{block}{Definition}
        The parity check (or, Hamming) matrix $H_c$ for an $n$-qubit code is a $m \times 2n$ matrix defined via concatenation as $H_c=[H_X | H_Z]$.
    \end{block}
\end{frame}

\begin{frame}{A five-qubit code (cont'd)}
    \begin{block}{Definition}
        The symplectic Gram matrix is defined as $\Omega = \begin{pmatrix}
            \O_n & I_n \\ I_n & \O_n
        \end{pmatrix}$.
    \end{block}
    \begin{block}{Definition}
        The symplectic error vector $\vec e=(e_x |e_z)$ represents the location of errors. In $e_x$, a 1 at index $i$ means an $X$ error occurred on physical qubit $i$, and likewise for $e_z$ and $Z$ errors.
    \end{block}
    \begin{block}{Definition}
        The symplectic syndrome vector $\vec s$ is defined as $\vec s = H_c\Omega\vec e^T$. Note that $\vec s$ is $m$-dimensional, corresponding to detection events of the $m$ stabilizers in the code. Given a syndrome vector $\vec s$, we must compute the error vector $\vec e$ to locate the $X$ and $Z$ errors on any of the $n$ physical qubits.
    \end{block}
\end{frame}

\begin{frame}{Spectral Theorem}

\begin{block}{Definition}
A diagonal matrix is a matrix such that all entries outside the main diagonal are zero.
\end{block}

\begin{block}{Definition}
The spectrum of a matrix $A$ is the set of all eigenvalues of $A$.
\end{block}

\begin{block}{Definition}
     A matrix $A$ is normal if $[A, A^\dagger]=\mathcal O$.
\end{block}

\begin{block}{Theorem}
If a matrix $A$ is normal, then $A$ is unitarily diagonalizable, meaning that $A=UDU^\dagger$ for some unitary matrix $U$ comprising the eigenvectors of $A$, and a diagonal matrix $D$, where the diagonal values of $D$ is the spectrum of $A$. Such a factorization is called the spectral decomposition of $A$.
\end{block}

\end{frame}

\begin{frame}{Spectral Theorem (cont'd)}

\begin{block}{Remark}
    The spectral decomposition of any $n \times n$ Hermitian matrix $A$ is
    $$ A = \sum_{k=1}^n\lambda_k \ket{\psi}_k\bra{\psi}_k , $$
    where $\lambda_k$ is an eigenvalue of $A$ and $\ket{\psi}_k$ is its corresponding orthonormal eigenstate.
\end{block}

\begin{block}{Example}
    The spectral theorem yields the following decomposition of the Pauli $Z$ matrix in terms of projection operators:
    $$ Z = \begin{pmatrix}
        1 & 0 \\ 0 & -1
    \end{pmatrix} = (1)\hat{P}_1 + (-1)\hat{P}_{-1} $$ since the eigenvalues of $Z$ are $+1$ and $-1$.
\end{block}
\end{frame}
\begin{frame}{Spectral Theorem (cont'd)}

\begin{block}{Remark}
The spectral theorem tells us that, since the Pauli matrices $\{X,Y,Z\}$ are Hermitian, their eigenvalues are $\pm 1$ and their eigenvectors form a mutually orthogonal and complete basis of the qubit state space $\C^2$, justifying their use as the fundamental observables of a qubit. The Pauli matrices create a basis for all $2\times2$ Hermitian matrices.
\end{block}

\begin{block}{Exercise}
    Show that every unitary matrix and Hermitian matrix is normal.
\end{block}

\begin{flushright}
    \textit{I consider nature a vast chemical laboratory in which all kinds of composition and decompositions are formed.}\\---Antoine Lavoisier
\end{flushright}

\begin{flushright}
    \textit{This isn't right. This isn't even wrong.}\\---Wolfgang Pauli
\end{flushright}


\end{frame}


\begin{frame}{Linear functionals and dual spaces}
\begin{block}{Definition}
A linear functional $f$ is a mapping between elements of a vector space into  the complex plane (ie, $f:V\to \C$) such that $$f(\vec{u} + \vec{v}) = f(\vec u) + f(\vec v) \text{   and   }f(\alpha\vec u) = \alpha f(\vec u)$$ for any $\vec u, \vec v\in V$ and $\alpha \in \C$.
\end{block}

\begin{block}{Definition}
The dual space of a vector field $V$ is the set of all linear functionals over $V$.
\end{block}

\begin{block}{Remark}
The bra $\bra\phi$ is a member of the dual space of the vector space containing $\ket\phi$. As stated previously, the bra-ket represents the mapping of state vectors in $\C^n$ into $\C$.
\end{block}

\begin{flushright}
    \textit{Adding functionality is not just a matter of adding code.}\\---Wietse Venema
\end{flushright}

\end{frame}

\begin{frame}{Riesz Representation Theorem}


\begin{block}{Definition}
A linear map $T$ is anti-linear if $$T(\alpha\ket\phi + \beta\ket\psi) = \alpha^*T\ket\phi + \beta^*T\ket\psi.$$
\end{block}

\begin{block}{Definition}
Two vector spaces $V$ and $W$ are isomorphic if there exists a bijective linear map $T:V\to W$, meaning that
\begin{itemize}
    \item (it's injective) every unique vector in $V$ maps to a unique vector in $W$ (that is, if $T(\vec u) = T(\vec v)$ then $\vec u = \vec v$) and
    \item  (it's surjective) that for every vector $\vec w \in W$ there exists a vector $\vec v\in V$ such that $T(\vec v) = \vec w$.
\end{itemize}
\end{block}

\end{frame}

\begin{frame}{Riesz Representation Theorem (cont'd)}

\begin{block}{Definition}
An isomorphism is canonical if it is defined by the intrinsic mathematical properties of the vector spaces it acts on, that is, no additional change of basis matrix is necessary to define it.
\end{block}

\begin{block}{Theorem (Riesz)}
For any Hilbert space $\mathcal H$ there exists a unique canonical anti-linear isomorphism between $\mathcal H$ and its dual space $\mathcal H^*$.
\end{block}

\begin{block}{Remark}
The Riesz representation theorem is how we ensure a bijective, normalization-preserving correspondence between bras and kets; we will often write $\ket\psi = (\bra\psi)^\dagger$, where the Hermitian conjugate operator $\dagger$ represents the conversion between a row and a column vector and conjugation of vector elements.
\end{block}

\begin{flushright}
    \textit{We are servants rather than masters in mathematics.}\\---Charles Hermite
\end{flushright}

\end{frame}

\begin{frame}{Application: the uncertainty principle}
    \begin{block}{Definition}
        The expectation value of a Hermitian operator (or, observable) $A$ with respect to a particular normalized state vector $\ket\psi$ of equal dimension to $A$ is a scalar computed as: $$ \langle A \rangle = \bra\psi A \ket\psi $$
    \end{block}
    \begin{block}{Remark}
        The expectation value corresponds to the probabilistic expected value of an experiment measuring an observable $A$ on $\ket\psi$.
    \end{block}
    \begin{block}{Remark}
        The expectation value is analogous to the expected value of a random variable $X$ which has a probability density function $f(x)$ satisfying
        $$ p(a \leq X \leq b) = \int_a^b f(x)dx \text{ and } \mathbb E[X]= \int_{-\infty}^\infty xf(x)dx. $$
    \end{block}
\end{frame}


\begin{frame}{Application: the uncertainty principle (cont'd)}
    \begin{block}{Definition}
        The variance of an operator $A$, denoted $\Var(A)$, is computed as $\Var(A) = \langle A^2 \rangle - \langle A \rangle ^2$
    \end{block}
    \begin{block}{Definition}
        The standard deviation of an operator $A$, denoted $\Delta A$ or $\sigma_A$, is simply $\Delta A=\sqrt{\Var (A)}$.
    \end{block}
    \begin{block}{Theorem (Robertson)}
        For any two non-commuting Hermitian operators $A$ and $B$, we have
        $$ \Delta A \Delta B \geq \frac{1}{2} \left | \left\langle \left [A, B \right ] \right\rangle \right |. $$
    \end{block}
\end{frame}

\begin{frame}{Application: the uncertainty principle (cont'd)}
    \begin{block}{Exercise}
        Verify the Robertson uncertainty relation for each distinct pair of Pauli matrices. (Hint: compute the expectation values in terms of basis vectors.)
    \end{block}
    \begin{flushright}
        \textit{Information is the resolution of uncertainty.}\\---Claude Shannon
    \end{flushright}
\end{frame}

\end{document}